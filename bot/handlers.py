import logging
from telegram import Update
from telegram.ext import ContextTypes

from core.spam_detector import SpamDetector
from core.punishment_manager import PunishmentManager
from core.whitelist import WhitelistManager
from utils.rate_limiter import RateLimiter

logger = logging.getLogger("telegram_bot")


class MessageHandler:
    """è¨Šæ¯è™•ç†å™¨"""

    def __init__(
        self,
        spam_detector: SpamDetector,
        punishment_manager: PunishmentManager,
        whitelist_manager: WhitelistManager,
        rate_limiter: RateLimiter,
        dry_run: bool = False,
        enable_whitelist: bool = True
    ):
        self.spam_detector = spam_detector
        self.punishment = punishment_manager
        self.whitelist = whitelist_manager
        self.rate_limiter = rate_limiter
        self.dry_run = dry_run
        self.enable_whitelist = enable_whitelist

        if self.dry_run:
            logger.warning("ğŸ”§ DRY RUN MODE ENABLED - No actions will be taken, only logging")
        if not self.enable_whitelist:
            logger.warning("âš ï¸ WHITELIST DISABLED - All messages will be checked, including admins")

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """
        è™•ç†ç¾¤çµ„è¨Šæ¯

        æª¢æŸ¥æµç¨‹ï¼š
        1. æª¢æŸ¥æ˜¯å¦ç‚ºç™½åå–®ç”¨æˆ¶
        2. æª¢æŸ¥ API é…é¡
        3. ä½¿ç”¨ LLM æª¢æ¸¬åƒåœ¾è¨Šæ¯
        4. åŸ·è¡Œç›¸æ‡‰è™•ç½°
        """
        message = update.message

        # å¿½ç•¥éæ–‡å­—è¨Šæ¯
        if not message or not message.text:
            return

        # å¿½ç•¥æŒ‡ä»¤
        if message.text.startswith('/'):
            return

        user_id = message.from_user.id
        username = message.from_user.username or message.from_user.first_name
        message_text = message.text

        logger.debug(f"Processing message from user {user_id} ({username}): {message_text[:50]}...")

        # Debug: æª¢æŸ¥è¨Šæ¯çš„å›è¦†ç‹€æ…‹
        logger.debug(
            f"Message reply info: "
            f"is_topic_message={message.is_topic_message}, "
            f"reply_to_message={'exists' if message.reply_to_message else 'None'}, "
            f"message_thread_id={message.message_thread_id}"
        )

        # æª¢æ¸¬æ˜¯å¦å›è¦†äº†ä¸å­˜åœ¨æ–¼æœ¬ç¾¤çµ„çš„è¨Šæ¯ï¼ˆè·¨ç¾¤çµ„å›è¦†ï¼Œå¸¸è¦‹æ–¼åƒåœ¾è¨Šæ¯ï¼‰
        is_cross_group_reply = False

        # æƒ…æ³1: reply_to_message å­˜åœ¨ä¸”ä¾†è‡ªä¸åŒç¾¤çµ„
        if message.reply_to_message:
            replied_chat_id = message.reply_to_message.chat.id
            current_chat_id = message.chat.id

            if replied_chat_id != current_chat_id:
                is_cross_group_reply = True
                logger.warning(
                    f"Detected cross-group reply (case 1) from user {user_id}: "
                    f"current_chat={current_chat_id}, replied_chat={replied_chat_id}"
                )

        # æƒ…æ³2: è¨Šæ¯çœ‹èµ·ä¾†åƒå›è¦†ä½† reply_to_message æ˜¯ Noneï¼ˆå›è¦†ä¸å­˜åœ¨çš„è¨Šæ¯ï¼‰
        # æª¢æŸ¥è¨Šæ¯æ˜¯å¦æœ‰ reply_to_message_id ä½†æ²’æœ‰ reply_to_message ç‰©ä»¶
        elif hasattr(message, 'reply_to_message_id') and message.reply_to_message_id:
            is_cross_group_reply = True
            logger.warning(
                f"Detected cross-group reply (case 2) from user {user_id}: "
                f"has reply_to_message_id={message.reply_to_message_id} but reply_to_message is None"
            )

        # æª¢æŸ¥ç™½åå–®ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.enable_whitelist and self.whitelist.is_whitelisted(user_id):
            logger.debug(f"User {user_id} is whitelisted, skipping check")
            return

        # å¦‚æœæª¢æ¸¬åˆ°è·¨ç¾¤çµ„å›è¦†ï¼Œç›´æ¥åˆ¤å®šç‚ºåƒåœ¾è¨Šæ¯
        if is_cross_group_reply:
            logger.warning(f"Auto-detected spam: cross-group reply from user {user_id}")
            if self.dry_run:
                logger.warning(
                    f"ğŸ” [DRY RUN] Cross-group reply spam detected! user={user_id}, username={username}\n"
                    f"Message: {message_text}"
                )
            else:
                action = await self.punishment.handle_spam(
                    user_id=user_id,
                    username=username,
                    message=message,
                    llm_score=10.0  # æœ€é«˜åˆ†ï¼Œç›´æ¥åˆ¤å®šç‚ºåƒåœ¾è¨Šæ¯
                )
                logger.warning(
                    f"ğŸš¨ Cross-group reply spam! user={user_id}, username={username}, action={action}"
                )
            return

        # æª¢æŸ¥ API é…é¡
        can_call = await self.rate_limiter.can_call_api()
        if not can_call:
            remaining = await self.rate_limiter.get_remaining()
            logger.warning(
                f"Daily API limit reached, skipping detection for message from user {user_id}"
            )
            # é”åˆ°ä¸Šé™å¾Œä¸æª¢æ¸¬ï¼Œä½†è¨˜éŒ„æ—¥èªŒ
            if remaining == 0:
                # åªåœ¨ç¬¬ä¸€æ¬¡é”åˆ°ä¸Šé™æ™‚è¨˜éŒ„
                logger.error("âš ï¸ API daily limit reached! No more spam detection today.")
            return

        # ä½¿ç”¨ LLM æª¢æ¸¬åƒåœ¾è¨Šæ¯
        try:
            is_spam, score, reasoning = await self.spam_detector.check_message(message_text)
            await self.rate_limiter.increment()

            logger.info(
                f"Message checked: user={user_id}, username={username}, "
                f"score={score:.1f}, is_spam={is_spam}, reasoning={reasoning}"
            )

            # å¦‚æœæ˜¯åƒåœ¾è¨Šæ¯ï¼ŒåŸ·è¡Œè™•ç½°
            if is_spam:
                if self.dry_run:
                    # Dry Run æ¨¡å¼ï¼šåªè¨˜éŒ„ï¼Œä¸åŸ·è¡Œè™•ç½°
                    logger.warning(
                        f"ğŸ” [DRY RUN] Spam detected! user={user_id}, username={username}, "
                        f"score={score:.1f}, reasoning={reasoning}\n"
                        f"Message: {message_text}"
                    )
                else:
                    # æ­£å¸¸æ¨¡å¼ï¼šåŸ·è¡Œè™•ç½°
                    action = await self.punishment.handle_spam(
                        user_id=user_id,
                        username=username,
                        message=message,
                        llm_score=score
                    )
                    logger.warning(
                        f"ğŸš¨ Spam detected! user={user_id}, username={username}, "
                        f"score={score:.1f}, action={action}"
                    )

        except Exception as e:
            logger.error(
                f"Error processing message from user {user_id}: {e}",
                exc_info=True
            )
            # éŒ¯èª¤æ™‚ä¸è™•ç½°ç”¨æˆ¶ï¼Œé¿å…èª¤åˆ¤

    async def handle_photo(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """
        è™•ç†ç¾¤çµ„åœ–ç‰‡è¨Šæ¯ï¼ˆåŒ…æ‹¬è½‰ç™¼çš„åœ–ç‰‡ï¼‰

        æª¢æŸ¥æµç¨‹ï¼š
        1. æª¢æŸ¥æ˜¯å¦ç‚ºç™½åå–®ç”¨æˆ¶
        2. æª¢æŸ¥ API é…é¡
        3. ä¸‹è¼‰åœ–ç‰‡
        4. ä½¿ç”¨ LLM Vision API æª¢æ¸¬åƒåœ¾åœ–ç‰‡ï¼ˆå¦‚åˆç´„æ›¬å–®ï¼‰
        5. åŸ·è¡Œç›¸æ‡‰è™•ç½°
        """
        message = update.message

        # ç¢ºä¿è¨Šæ¯æœ‰åœ–ç‰‡
        if not message or not message.photo:
            return

        user_id = message.from_user.id
        username = message.from_user.username or message.from_user.first_name
        caption = message.caption or ""

        logger.debug(f"Processing photo from user {user_id} ({username}), caption: {caption[:50] if caption else 'No caption'}...")

        # æª¢æŸ¥ç™½åå–®ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.enable_whitelist and self.whitelist.is_whitelisted(user_id):
            logger.debug(f"User {user_id} is whitelisted, skipping check")
            return

        # æª¢æŸ¥ API é…é¡
        can_call = await self.rate_limiter.can_call_api()
        if not can_call:
            remaining = await self.rate_limiter.get_remaining()
            logger.warning(
                f"Daily API limit reached, skipping photo detection for user {user_id}"
            )
            if remaining == 0:
                logger.error("âš ï¸ API daily limit reached! No more spam detection today.")
            return

        # ä¸‹è¼‰åœ–ç‰‡ä¸¦æª¢æ¸¬
        try:
            # å–å¾—æœ€å¤§å°ºå¯¸çš„åœ–ç‰‡
            photo = message.photo[-1]
            photo_file = await context.bot.get_file(photo.file_id)

            # ä¸‹è¼‰åœ–ç‰‡åˆ°è¨˜æ†¶é«”
            import io
            photo_bytes = io.BytesIO()
            await photo_file.download_to_memory(photo_bytes)
            photo_data = photo_bytes.getvalue()

            # ä½¿ç”¨ LLM Vision æª¢æ¸¬åœ–ç‰‡
            is_spam, score, reasoning = await self.spam_detector.check_image(
                image_data=photo_data,
                caption=caption if caption else None
            )
            await self.rate_limiter.increment()

            logger.info(
                f"Photo checked: user={user_id}, username={username}, "
                f"score={score:.1f}, is_spam={is_spam}, reasoning={reasoning}"
            )

            # å¦‚æœæ˜¯åƒåœ¾è¨Šæ¯ï¼ŒåŸ·è¡Œè™•ç½°
            if is_spam:
                if self.dry_run:
                    # Dry Run æ¨¡å¼ï¼šåªè¨˜éŒ„ï¼Œä¸åŸ·è¡Œè™•ç½°
                    logger.warning(
                        f"ğŸ” [DRY RUN] Spam photo detected! user={user_id}, username={username}, "
                        f"score={score:.1f}, reasoning={reasoning}\n"
                        f"Caption: {caption}"
                    )
                else:
                    # æ­£å¸¸æ¨¡å¼ï¼šåŸ·è¡Œè™•ç½°
                    action = await self.punishment.handle_spam(
                        user_id=user_id,
                        username=username,
                        message=message,
                        llm_score=score
                    )
                    logger.warning(
                        f"ğŸš¨ Spam photo detected! user={user_id}, username={username}, "
                        f"score={score:.1f}, action={action}"
                    )

        except Exception as e:
            logger.error(
                f"Error processing photo from user {user_id}: {e}",
                exc_info=True
            )
            # éŒ¯èª¤æ™‚ä¸è™•ç½°ç”¨æˆ¶ï¼Œé¿å…èª¤åˆ¤
